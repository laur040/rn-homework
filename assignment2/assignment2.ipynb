{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2l9CrXbH_Jw",
        "outputId": "ae29a171-a6ae-44f0-8a09-3d2326efd341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 181274683.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 25839525.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 88533608.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4334591.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torchvision.datasets import MNIST\n",
        "def download_mnist(is_train: bool):\n",
        "    dataset = MNIST(root='./data',\n",
        "            transform=lambda x: np.array(x).flatten(),\n",
        "            download=True,\n",
        "            train=is_train)\n",
        "    mnist_data = []\n",
        "    mnist_labels = []\n",
        "    for image, label in dataset:\n",
        "        mnist_data.append(image)\n",
        "        mnist_labels.append(label)\n",
        "    return mnist_data, mnist_labels\n",
        "train_X, train_Y = download_mnist(True)\n",
        "test_X, test_Y = download_mnist(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the data and convert the labels to one-hot-encoding"
      ],
      "metadata": {
        "id": "P2UdAUVoIyIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = np.array(train_X) / 255.0\n",
        "test_X = np.array(test_X) / 255.0"
      ],
      "metadata": {
        "id": "E0b4Tj4FIOaI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(labels):\n",
        "  # labels = labels.astype(int)\n",
        "  one_hot_encode = np.zeros((len(labels), 10))\n",
        "  for i in range (len(labels)):\n",
        "    one_hot_encode[i, labels[i]] = 1\n",
        "  return one_hot_encode\n",
        "\n",
        "train_Y = one_hot_encoding(train_Y)\n",
        "test_Y = one_hot_encoding(test_Y)"
      ],
      "metadata": {
        "id": "ID-roNiJIbUi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_param(input, output):\n",
        "  W = np.random.rand(input, output)\n",
        "  b = np.zeros((output,))\n",
        "  return W, b\n",
        "\n",
        "#print(init_param(784, 10))\n"
      ],
      "metadata": {
        "id": "DOYq2dGMJzw8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax, Cross Entrpy Loss, Gradient Descent"
      ],
      "metadata": {
        "id": "yFwD0I3ZJdBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  exp_x = np.exp(x - np.max(x, axis = 1, keepdims = True))\n",
        "  return exp_x / np.sum(exp_x, axis = 1, keepdims = True)"
      ],
      "metadata": {
        "id": "8rMHCx4RL5R2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(true_y, prediction_y):\n",
        "  m = true_y.shape[0]\n",
        "  log_p = -np.log(prediction_y[range(m), true_y.argmax(axis=1)])\n",
        "  loss = np.sum(log_p) / m\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "JhMb3IA3P2t5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, true_y, prediction_y, W, b, learning_rate):\n",
        "  error = true_y - prediction_y\n",
        "  W += learning_rate * (np.dot(X.T, error))\n",
        "  b += learning_rate * np.sum(error, axis = 0)\n",
        "  return W, b"
      ],
      "metadata": {
        "id": "IwI5WvpUXEMl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction, Accuracy"
      ],
      "metadata": {
        "id": "8S1Pw4jRJcpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W, b):\n",
        "  z = np.dot(X, W) + b\n",
        "  prediction_y = softmax(z)\n",
        "  #return prediction_y\n",
        "  return np.argmax(prediction_y, axis = 1)"
      ],
      "metadata": {
        "id": "2YLPxJj8O8Zc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(X, Y, W, b):\n",
        "  prediction_y = predict(X, W, b)\n",
        "  true_y = np.argmax(Y, axis = 1)\n",
        "\n",
        "  acc = np.mean(prediction_y == true_y)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "C3aScSVEcNrq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "0mX8dfOWJsy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_X, train_Y, W, b, epochs=100, batch_size=100, learning_rate=0.01):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    i = np.random.permutation(train_X.shape[0])\n",
        "    train_X_random = train_X[i]\n",
        "    train_Y_random = train_Y[i]\n",
        "\n",
        "    for i in range(0, train_X.shape[0], batch_size):\n",
        "      batch_X = train_X_random[i : i + batch_size]\n",
        "      batch_Y = train_Y_random[i : i + batch_size]\n",
        "\n",
        "      #prediction_y = predict(batch_X, W, b)\n",
        "      z = np.dot(batch_X, W) + b\n",
        "      prediction_y = softmax(z)\n",
        "\n",
        "      loss = cross_entropy(batch_Y, prediction_y)\n",
        "\n",
        "      W, b = gradient_descent(batch_X, batch_Y, prediction_y, W, b, learning_rate)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} Loss: {loss}\")\n",
        "\n",
        "  return W, b\n"
      ],
      "metadata": {
        "id": "tRY6JQkfYRtO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W, b = init_param(784,10)\n",
        "trained_W, trained_b = train(train_X, train_Y, W, b, 100, 100, 0.01)\n",
        "\n",
        "test_acc = accuracy(test_X, test_Y, trained_W, trained_b)\n",
        "print(f\"Test accuracy: {test_acc * 100:.2f}% \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKdwMhCidtyi",
        "outputId": "5c6d1cf9-8ba4-4a6a-b277-f23dfbded146"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 0.569490353873796\n",
            "Epoch: 2 Loss: 0.21076225738622154\n",
            "Epoch: 3 Loss: 0.26924263306954244\n",
            "Epoch: 4 Loss: 0.1368058070604183\n",
            "Epoch: 5 Loss: 0.24766449538141724\n",
            "Epoch: 6 Loss: 0.3486496826360361\n",
            "Epoch: 7 Loss: 0.22865669189248664\n",
            "Epoch: 8 Loss: 0.348208976895903\n",
            "Epoch: 9 Loss: 0.21967654637165104\n",
            "Epoch: 10 Loss: 0.4757673895595647\n",
            "Epoch: 11 Loss: 0.2455511220487835\n",
            "Epoch: 12 Loss: 0.3962336028256068\n",
            "Epoch: 13 Loss: 0.2469593089874096\n",
            "Epoch: 14 Loss: 0.25239733145827126\n",
            "Epoch: 15 Loss: 0.27042291359455656\n",
            "Epoch: 16 Loss: 0.38059736472938427\n",
            "Epoch: 17 Loss: 0.4438110055504936\n",
            "Epoch: 18 Loss: 0.21105555183170627\n",
            "Epoch: 19 Loss: 0.17278968951649248\n",
            "Epoch: 20 Loss: 0.14361617477271277\n",
            "Epoch: 21 Loss: 0.32285933027794195\n",
            "Epoch: 22 Loss: 0.20837129935853888\n",
            "Epoch: 23 Loss: 0.23148352612397965\n",
            "Epoch: 24 Loss: 0.1771309348197768\n",
            "Epoch: 25 Loss: 0.23119851610319406\n",
            "Epoch: 26 Loss: 0.3559026437851414\n",
            "Epoch: 27 Loss: 0.21313681629107994\n",
            "Epoch: 28 Loss: 0.2750951610993228\n",
            "Epoch: 29 Loss: 0.3422639428581312\n",
            "Epoch: 30 Loss: 0.335224471794534\n",
            "Epoch: 31 Loss: 0.30730997964316553\n",
            "Epoch: 32 Loss: 0.18311721207850723\n",
            "Epoch: 33 Loss: 0.43277282441715426\n",
            "Epoch: 34 Loss: 0.23280732740865587\n",
            "Epoch: 35 Loss: 0.25822943332261883\n",
            "Epoch: 36 Loss: 0.27222701192835774\n",
            "Epoch: 37 Loss: 0.21505507579148705\n",
            "Epoch: 38 Loss: 0.3665640522145503\n",
            "Epoch: 39 Loss: 0.24146963038984467\n",
            "Epoch: 40 Loss: 0.2424490979518354\n",
            "Epoch: 41 Loss: 0.31944965874008735\n",
            "Epoch: 42 Loss: 0.18896170747679913\n",
            "Epoch: 43 Loss: 0.2736910282129764\n",
            "Epoch: 44 Loss: 0.2869961023043962\n",
            "Epoch: 45 Loss: 0.34871555024371703\n",
            "Epoch: 46 Loss: 0.06315690165611584\n",
            "Epoch: 47 Loss: 0.14943403914431005\n",
            "Epoch: 48 Loss: 0.14242001966692042\n",
            "Epoch: 49 Loss: 0.20619685123606843\n",
            "Epoch: 50 Loss: 0.288204785532345\n",
            "Epoch: 51 Loss: 0.21350599032033138\n",
            "Epoch: 52 Loss: 0.39791539205982285\n",
            "Epoch: 53 Loss: 0.22691039996320916\n",
            "Epoch: 54 Loss: 0.33341833751864675\n",
            "Epoch: 55 Loss: 0.161876599342175\n",
            "Epoch: 56 Loss: 0.32928151416035406\n",
            "Epoch: 57 Loss: 0.24377824917440719\n",
            "Epoch: 58 Loss: 0.3155737956064187\n",
            "Epoch: 59 Loss: 0.18445751759732626\n",
            "Epoch: 60 Loss: 0.30840084999383877\n",
            "Epoch: 61 Loss: 0.4061648608757502\n",
            "Epoch: 62 Loss: 0.23540114177670812\n",
            "Epoch: 63 Loss: 0.27149019536811236\n",
            "Epoch: 64 Loss: 0.21218425323886694\n",
            "Epoch: 65 Loss: 0.20541797068752154\n",
            "Epoch: 66 Loss: 0.2418622131081894\n",
            "Epoch: 67 Loss: 0.20513582976704126\n",
            "Epoch: 68 Loss: 0.3801378041015028\n",
            "Epoch: 69 Loss: 0.17716301497247208\n",
            "Epoch: 70 Loss: 0.1911314442384793\n",
            "Epoch: 71 Loss: 0.21389383897519645\n",
            "Epoch: 72 Loss: 0.21111824185910474\n",
            "Epoch: 73 Loss: 0.14668650022642246\n",
            "Epoch: 74 Loss: 0.2616456987190131\n",
            "Epoch: 75 Loss: 0.2027803566363955\n",
            "Epoch: 76 Loss: 0.17858311927110926\n",
            "Epoch: 77 Loss: 0.13920307397527884\n",
            "Epoch: 78 Loss: 0.23542605602385855\n",
            "Epoch: 79 Loss: 0.1962106434930671\n",
            "Epoch: 80 Loss: 0.180024320582714\n",
            "Epoch: 81 Loss: 0.16721060815637206\n",
            "Epoch: 82 Loss: 0.18620110039349855\n",
            "Epoch: 83 Loss: 0.3720153324115828\n",
            "Epoch: 84 Loss: 0.32945098751438495\n",
            "Epoch: 85 Loss: 0.24161109950688342\n",
            "Epoch: 86 Loss: 0.251401058567608\n",
            "Epoch: 87 Loss: 0.15867221060160383\n",
            "Epoch: 88 Loss: 0.4027812848377545\n",
            "Epoch: 89 Loss: 0.1956443488192158\n",
            "Epoch: 90 Loss: 0.2547664662535685\n",
            "Epoch: 91 Loss: 0.23039369759880818\n",
            "Epoch: 92 Loss: 0.3121578133592815\n",
            "Epoch: 93 Loss: 0.3371115774669577\n",
            "Epoch: 94 Loss: 0.3530126292754425\n",
            "Epoch: 95 Loss: 0.17845252553634824\n",
            "Epoch: 96 Loss: 0.22068917704696672\n",
            "Epoch: 97 Loss: 0.2981643178128576\n",
            "Epoch: 98 Loss: 0.18225512071877825\n",
            "Epoch: 99 Loss: 0.2579620821887437\n",
            "Epoch: 100 Loss: 0.3074369455401527\n",
            "Test accuracy: 92.19% \n"
          ]
        }
      ]
    }
  ]
}